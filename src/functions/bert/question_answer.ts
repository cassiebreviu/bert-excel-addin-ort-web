/* eslint-disable no-undef */
/**
This is an augmented/updated version of this: https://github.com/tensorflow/tfjs-models/blob/master/qna/src/question_and_answer.ts
 */
import { CLS_INDEX, loadTokenizer, BertTokenizer, SEP_INDEX, Token } from "./bert_tokenizer";

const MAX_ANSWER_LEN = 32;
const MAX_QUERY_LEN = 64;
const MAX_SEQ_LEN = 384;
const PREDICT_ANSWER_NUM = 5;
// This is threshold value for determining if a question is irrelevant to the
// context. This value comes from the QnA model, and is generated by the
// training process based on the SQUaD 2.0 dataset.
const NO_ANSWER_THRESHOLD = 4.3980759382247925;

/**
 * Answer object returned by the model.
 * `text`: string, the text of the answer.
 * `startIndex`: number, the index of the starting character of the answer in
 *     the passage.
 * `endIndex`: number, index of the last character of the answer text.
 * `score`: number, indicates the confident
 * level.
 */
export interface Answer {
  text: string;
  startIndex: number;
  endIndex: number;
  score: number;
}

export interface Feature {
  input_ids: Array<any>;
  input_mask: Array<any>;
  segment_ids: Array<any>;
  origTokens: Token[];
  tokenToOrigMap: { [key: number]: number };
}

export interface AnswerIndex {
  start: number;
  end: number;
  score: number;
}

export async function create_model_input(
  query: string,
  context: string,
  maxQueryLen: number = MAX_QUERY_LEN,
  maxSeqLen: number = MAX_SEQ_LEN,
  docStride = 128
): Promise<Feature> {
  //load tokenizer
  console.log("Loading tokenizer...");
  const tokenizer: BertTokenizer = await loadTokenizer();
  query = query.replace(/\?/g, "");
  query = query.trim();
  query = query + "?";
  const queryTokens = (await tokenizer).tokenize(query);
  if (queryTokens.length > maxQueryLen) {
    throw new Error(`The length of question token exceeds the limit (${maxQueryLen}).`);
  }
  console.log("Query tokens: ", queryTokens);
  const origTokens = tokenizer.processInput(context.trim());
  console.log("Original Tokens", origTokens);
  const tokenToOrigIndex: number[] = [];
  const allDocTokens: number[] = [];
  for (let i = 0; i < origTokens.length; i++) {
    const token = origTokens[i].text;
    const subTokens = tokenizer.tokenize(token);
    for (let j = 0; j < subTokens.length; j++) {
      const subToken = subTokens[j];
      tokenToOrigIndex.push(i);
      allDocTokens.push(subToken);
    }
  }

  // The -3 accounts for [CLS], [SEP] and [SEP]
  const maxContextLen = maxSeqLen - queryTokens.length - 3;
  // We can have documents that are longer than the maximum sequence
  // length. To deal with this we do a sliding window approach, where we
  // take chunks of the up to our max length with a stride of
  // `doc_stride`.
  const docSpans: Array<{ start: number; length: number }> = [];
  let startOffset = 0;
  while (startOffset < allDocTokens.length) {
    let length = allDocTokens.length - startOffset;
    if (length > maxContextLen) {
      length = maxContextLen;
    }
    docSpans.push({ start: startOffset, length });
    if (startOffset + length === allDocTokens.length) {
      break;
    }
    startOffset += Math.min(length, docStride);
  }
  const features = docSpans.map((docSpan) => {
    const tokens = [];
    const segment_ids = [];
    const tokenToOrigMap: { [index: number]: number } = {};
    tokens.push(CLS_INDEX);
    segment_ids.push(0);
    for (let i = 0; i < queryTokens.length; i++) {
      const queryToken = BigInt(queryTokens[i]);
      tokens.push(queryToken);
      segment_ids.push(0);
    }
    tokens.push(SEP_INDEX);
    segment_ids.push(0);
    for (let i = 0; i < docSpan.length; i++) {
      const splitTokenIndex = i + docSpan.start;
      const docToken = allDocTokens[splitTokenIndex];
      tokens.push(docToken);
      segment_ids.push(1);
      tokenToOrigMap[tokens.length] = tokenToOrigIndex[splitTokenIndex];
    }
    tokens.push(SEP_INDEX);
    segment_ids.push(1);
    const input_ids: any[] = tokens;
    const input_mask: any[] = input_ids.map(() => BigInt(1));
    // while (input_ids.length < maxSeqLen) {
    //   input_ids.push(BigInt(0));
    //   input_mask.push(BigInt(0));
    //   segment_ids.push(BigInt(0));
    // }
    console.log("Input Ids", input_ids);
    console.log("Input Mask", input_mask);
    console.log("Segment Ids", segment_ids);
    return { input_ids, input_mask, segment_ids, origTokens, tokenToOrigMap };
  });
  console.log("Features", features);
  return features[0];
}

/**
 * Find the Best N answers & logits from the logits array and input feature.
 * @param startLogits start index for the answers
 * @param endLogits end index for the answers
 * @param origTokens original tokens of the passage
 * @param tokenToOrigMap token to index mapping
 */
export function getBestAnswers(
  startLogits: number[],
  endLogits: number[],
  origTokens: Token[],
  tokenToOrigMap: { [key: string]: number },
  context: string
): Answer[] {
  // Model uses the closed interval [start, end] for indices.
  const startIndexes = getBestIndex(startLogits);
  const endIndexes = getBestIndex(endLogits);
  const origResults: AnswerIndex[] = [];
  startIndexes.forEach((start) => {
    endIndexes.forEach((end) => {
      if (tokenToOrigMap[start] && tokenToOrigMap[end] && end >= start) {
        const length = end - start;
        if (length < MAX_ANSWER_LEN) {
          origResults.push({ start, end, score: startLogits[start] + endLogits[end] });
        }
      }
    });
  });
  origResults.sort((a, b) => b.score - a.score);
  const answers: Answer[] = [];
  for (let i = 0; i < origResults.length; i++) {
    if (i >= PREDICT_ANSWER_NUM || origResults[i].score < NO_ANSWER_THRESHOLD) {
      break;
    }
    let convertedText = "";
    let startIndex = 0;
    let endIndex = 0;
    if (origResults[i].start > 0) {
      [convertedText, startIndex, endIndex] = convertBack(
        origTokens,
        tokenToOrigMap,
        origResults[i].start,
        origResults[i].end,
        context
      );
    } else {
      convertedText = "";
    }
    answers.push({
      text: convertedText,
      score: origResults[i].score,
      startIndex,
      endIndex,
    });
  }
  return answers;
}
/** Get the n-best logits from a list of all the logits. */
export function getBestIndex(logits: number[]): number[] {
  const tmpList = [];
  for (let i = 0; i < logits.length; i++) {
    tmpList.push([i, i, logits[i]]);
  }
  tmpList.sort((a, b) => b[2] - a[2]);
  const indexes = [];
  for (let i = 0; i < PREDICT_ANSWER_NUM; i++) {
    indexes.push(tmpList[i][0]);
  }
  return indexes;
}
/** Convert the answer back to original text form. */
export function convertBack(
  origTokens: Token[],
  tokenToOrigMap: { [key: string]: number },
  start: number,
  end: number,
  context: string
): [string, number, number] {
  const startIndex = tokenToOrigMap[start];
  const endIndex = tokenToOrigMap[end];
  const startCharIndex = origTokens[startIndex].index;
  const endCharIndex =
    endIndex < origTokens.length - 1
      ? origTokens[endIndex + 1].index - 1
      : origTokens[endIndex].index + origTokens[endIndex].text.length;
  return [context.slice(startCharIndex, endCharIndex + 1).trim(), startCharIndex, endCharIndex];
}
